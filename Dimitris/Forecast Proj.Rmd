---
title: "AFCS proj"
output: html_document
date: "2023-12-05"
---
```{r, echo=FALSE}
library(vroom)
library(stringr)
library(dplyr)
library(tidyverse)
library(kableExtra)
library(plotly)
library(zoo)
library(ggthemes)
library(ggplot2)
library(gridExtra)
library(lubridate)
```


Quick loading the datasets
```{r}
#---------------------LOAD DATASETS---------------------
path = 'C:/Users/dimts/Downloads/'

sell_prices <- vroom(str_c(path,"sell_prices_afcs2023.csv"), delim = ",", col_types = cols())

sales_train_validation <- vroom(str_c(path,"sales_train_validation_afcs2023.csv"), delim = ",", col_types = cols())

calendar <- vroom(str_c(path,"calendar_afcs2023.csv"), delim = ",", col_types = cols())

#sales_test_validation <- vroom(str_c(path,"sales_test_validation_afcs2022.csv"), delim = ",", col_types = cols())

#samples_submission <- vroom(str_c(path,"sample_submission_afcs2023.csv"), delim = ",", col_types = cols())

```

Check datasets - similar to opening csv file
```{r}
#quick look at datasets
file <- sales_train_validation #calendar , sales_train_validation , sell_prices
file %>% 
  select(seq(1,10,1)) %>% 
  head(10) %>% 
  kable() %>% 
  kable_styling()
```

By observing the Time series for total sales, we notice an increasing Trend in total sales, with obvious weekly seasonality. Most noticeably, there are 5 troughs in sales that have yearly seasonality and occur near the end of the year. Other than that, there are random spikes throughout the series that could be circumstantial. The overall variation is constant, so it does not increase with the level of the series. 
```{r}
extract_ts <- function(df){
  min_date <- as.Date("2011-01-29") #lowest data, corresponds to d_1
  df %>%
    select(id, starts_with("d_")) %>%   #take the id of the item and the daily sales columns
    pivot_longer(starts_with("d_"), names_to = "dates", values_to = "sales") %>%     #reshape
    mutate(dates = as.integer(str_remove(dates, "d_"))) %>%   
    mutate(dates = min_date + dates - 1) %>%         #turn day numbers into dates
    mutate(id = str_remove(id, "_validation"))       #remove end of item id
}

foo <- sales_train_validation %>%            #sum of columns for each day
  summarise_at(vars(starts_with("d_")), sum) %>% 
  mutate(id = 1)

bar <- extract_ts(foo)
gg <- bar %>% 
  ggplot(aes(dates, sales)) +
  geom_line(col = "blue") +
  theme_tufte() +
  labs(x = "Date", y = "Sales", title = "Total sales Time series")

ggplotly(gg, dynamicTicks = TRUE)
```

```{r}
sales_long <- sales_train_validation %>%
  select(id, starts_with("d_")) %>%  
  pivot_longer(starts_with("d_"), names_to = "dates", values_to = "sales") %>%
  mutate(dates = as.integer(str_remove(dates, "d_")))

plot <- sales_long %>%
  plot_ly(x = ~as.Date("2011-01-29") + dates - 1, y = ~sales, color = ~id, type = 'scatter', mode = 'lines') %>%
  layout(title = "Sales Over Time",
         xaxis = list(title = "Date"),
         yaxis = list(title = "Sales"),
         showlegend = TRUE,
         colorway = 'Set3')

plot
```

```{r}
#50 random items, more clear info
set.seed(123)  # Set seed for reproducibility
random_items <- sample(unique(sales_train_validation$id), 50)

sales_selected <- sales_train_validation %>%
  filter(id %in% random_items) %>%
  select(id, starts_with("d_")) %>%  
  pivot_longer(starts_with("d_"), names_to = "dates", values_to = "sales") %>%
  mutate(dates = as.integer(str_remove(dates, "d_")))

plot <- sales_selected %>%
  plot_ly(x = ~as.Date("2011-01-29") + dates - 1, y = ~sales, color = ~id, type = 'scatter', mode = 'lines') %>%
  layout(title = "Sales Over Time for 50 random items",
         xaxis = list(title = "Date"),
         yaxis = list(title = "Sales"),
         showlegend = TRUE,
         colorway = "Set3")

plot
```

By plotting the daily sales for each item we observe that there are many low values with some seemingly random spikes. These spikes, if they were random or similar to white noise would make our forecasts difficult. Another challenge is that not all products are available every day; some have gaps in their records, others start or stop being offered at different times in the studied period. For that reason we investigate further, using the calendar dataset which reveals information that might affect sales, such as annual events or promotions.

Merging happens here. First merge the calendar and sales_train to keep sales for every day for each item.
```{r}
#change column names from d_1 to dates
#also fix item_id
sales_train_validation_long <- sales_train_validation %>%
  pivot_longer(cols = -id, names_to = "date", values_to = "sales") %>%
  mutate(date = as.Date("2011-01-29") + as.integer(str_remove(date, "d_")) - 1) %>%
  rename(item_id = id) %>%
  mutate(item_id = str_remove(item_id, "_TX_3_validation"))
sales_train_validation_long
```

Now merge the above with the sell_prices. We should now have the daily data for each item's price and sales.
```{r}
calendar <- calendar %>%
  mutate(date = as.Date(date, format = "%m/%d/%Y"))

result <- merge(calendar, sales_train_validation_long, by = "date", all.x = TRUE)
result <- result %>%
  arrange(date, item_id)

merged_data <- result %>%
  left_join(sell_prices %>% select(wm_yr_wk, item_id, sell_price), by = c("wm_yr_wk", "item_id"))

merged_data
```




Heatmap for sales per day Mon-Sun per month:
We observe that Sales are higher during the weekend regardless of the month. Monday is still lower, but above the other 4 weekdays. The highest number of sales are measured on Sunday in the months of March and August. The lowest number of sales is recorded on Thursdays in November, followed closely by Wednesdays in January. The weekly seasonality is quite consistent in the Heat-map, too. 
```{r}
heatmap_data <- result %>%
  group_by(weekday, month = month(date, label = TRUE, abbr = FALSE)) %>%
  summarise(total_sales = sum(sales, na.rm = TRUE))
heatmap_data
# Create the heatmap using ggplot2
ggplot(heatmap_data, aes(x = factor(month, labels = month.name), y = factor(weekday, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")), fill = total_sales)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "blue") +
  labs(title = "Total Sales Heatmap",
       x = "Month",
       y = "Day of Week",
       fill = "Total Sales") +
  theme_minimal()
```

Upon analyzing the plots below, we observe that the majority (90%) of the recorded data corresponds to non-event days. Among the days with events, religious events are the most common, while sporting events are the least frequent.

WOULD BE INTERESTING TO INVESTIGATE RELATION BETWEEN EVENTS AND SALES. MAYBE SOME TYPE OF EVENT HAS HIGHER SALES THAN OTHERS. 
INTUITION 1: Create a new column for each event type and see correlations
INTUITION 2: See which specific items depend the most on which events. 
```{r}
na_event_percentage <- result %>%
  mutate(has_event = ifelse(is.na(event_name_1), "No Event", "Event")) %>%
  group_by(has_event) %>%
  summarise(percentage = n() / nrow(result) * 100)

p1 <- ggplot(na_event_percentage, aes(x = has_event, y = percentage, fill = has_event)) +
  geom_bar(stat = "identity") +
  labs(title = "Days with Events", x = "", y = "Percentage") +
  scale_y_continuous(labels = scales::percent_format(scale = 1)) +
  theme_minimal() +
  labs(fill = "Events")

#number that each event type occurs
event_counts <- result %>%
  filter(!is.na(event_name_1)) %>%
  group_by(event_type_1) %>%
  summarise(count = n())

p2 <- ggplot(event_counts, aes(x = event_type_1, y = count, fill = event_type_1)) +
  geom_bar(stat = "identity") +
  labs(title = "Occurence of Events", x = "", y = "Count") +
  theme_minimal() +
  labs(fill = "Event Type")

grid.arrange(p1, p2, ncol = 2)
```

Similarly, investigate the SNAP_TX column. The Snap_TX is 0 for approximately 67% of the days, so Snap purchases are not allowed for about 2/3 of the dates recorded. We also investigate the Distribution of event types for the days when Snap is offered and they look similar to the previous plot, except for the fact that Sporting is now tied in last place with Cultural events.

```{r}
snap_percentage <- result %>%
  group_by(snap_TX) %>%
  summarise(percentage = n() / nrow(result) * 100)

# Bar plot
ggplot(snap_percentage, aes(x = factor(snap_TX), y = percentage, fill = factor(snap_TX))) +
  geom_bar(stat = "identity") +
  labs(title = "Percentage of Days with 0s and 1s in snap_TX Column", x = "snap_TX", y = "Percentage") +
  scale_y_continuous(labels = scales::percent_format(scale = 1)) +
  theme_minimal()

snap_tx_1_data <- result[result$snap_TX == 1, ]
event_counts_snap_tx_1 <- snap_tx_1_data %>%
  filter(!is.na(event_name_1)) %>%
  group_by(event_type_1) %>%
  summarise(count = n())
ggplot(event_counts_snap_tx_1, aes(x = event_type_1, y = count, fill = event_type_1)) +
  geom_bar(stat = "identity") +
  labs(title = "Number of Days with Events by Event Type (SNAP_TX = 1)", x = "Event Type", y = "Count") +
  theme_minimal()
```







################ Trying autocorrelation but fails, Idk if it even makes sense.


By simply 
```{r}
result$event_type_1 <- ifelse(is.na(result$event_type_1), "Empty", result$event_type_1)
result$event_type_1_numeric <- as.numeric(factor(result$event_type_1, levels = c("Empty", "Cultural", "National", "Religious", "Sporting"), labels = c(0, 1, 2, 3, 4)))
event_type_counts <- table(result$event_type_1_numeric)
print(event_type_counts)

correlation_matrix <- cor(result[, c("sales", "wm_yr_wk", "wday", "month", "snap_TX", "sell_price", "event_type_1_numeric")])
correlation_df <- as.data.frame(as.table(correlation_matrix))
print(correlation_matrix)

ggplot(data = correlation_df, aes(x = Var1, y = Var2, fill = Freq)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "black") +
  theme_minimal() +
  labs(title = "Correlation Heatmap", x = "Variable", y = "Variable")
```


```{r}
result
```